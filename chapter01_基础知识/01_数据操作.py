# 在深度学习框架中，我们一般把n维数组称为张量(tensor)，它与numpy中的ndarray相似，但是张量支持GPU和CPU双重计算，且支持自动微分。
# 张量是由一个数值构成的数组。通常情况下，具有一个轴的张量被称为向量(vector)，两个轴的张量则是矩阵(matrix)
import torch

# 利用troch来创建一个行向量，该向量包含从0到12的所有整数（当然我们也可以创建浮点数）。新的张量默认会存储到内存（栈内存，因为计算速度快）中，并给予CPU进行计算。
# x=torch.arange(12)
# print(x)
# 查看向量的shape属性：张量中每个轴的长度和元素的总数
# print(x.shape)#torch.Size([12])
# 调用numel()方法：计算张量中元素的个数
# print(x.numel())#12
# x=x.reshape(3,4)
# print(x)
# x=x.reshape(6,-1)
# print(x)

# 创建一个两行三列四维的张量（实际中一般只用到二维张量），张量内数字全为0
# print(torch.zeros(2,3,4))
# 创建一个三行四列二维的张量，张量内数字全为1
# print(torch.ones(3,4,2))

# 当我们在构造神经网络，可能需要一个随机数组来作为初始化参数的值。
# 注意：使用此方法创造的矩阵中的数是满足标准高斯分布的（正态分布），即每个数字都是从均值为0，标准差为1的样本中随机取样的
# print(torch.randn(3,4))

# 也可以直接给一些确定的值来构成张量
# print(torch.tensor([[1,2,3],[3,4,6],[9,5,4]]))

# 接下来讲讲运算符
# 在诸多运算符中，最简单的运算是针对元素进行的运算，而矩阵的运算，就是矩阵中各个元素的运算。

# 1.对张量中所有元素进行求和
# print(torch.ones(3,4,2).sum())

# # 2.基本运算：加减乘除幂-- +，-，*，/，** 
# x=torch.tensor([1,2,3,5])
# y=torch.tensor([8,4,7,3])
# # print(x+y)
# # print(x*y)
# # print(x/y)
# # print(x**y)#([   1,   16, 2187,  125])  可以看到是y中每个元素的x次幂
# # print(y**x)#([  8,  16, 343, 243])
# # # 求以e为底的x次方
# # print(torch.exp(x))
# # 张量的连接,dim为选择的扩维
# # print(torch.cat((x,y),dim=0))
# x = torch.tensor([[4.0, 8, 1, 6], [5, 1, 0, 5], [7, 4, 3, 6]])
# y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
# # print(torch.cat((x,y),dim=1))
# print(x==y)


# 某些情况下，即使两个张量的形状不同，我们仍然可以通过广播机制对张量的元素进行操作
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
# print(a)
# print(b)
# # 可以看到：a是一个3*1的张量，b是一个1*2的向量。此时如果让a+b，则会进行广播:a会生成一个一模一样的列，b会生成两个一模一样的行，这样就有两个3*2的张量来运算了。
# print(a+b)
# # 对*同理
# print(a*b)


# 关于内存：由于在机器学习中，参数的大小可能达到几百M上G，为了节省内存空间，请不要频繁地更换变量代表的向量的地址值
# z=a*b
# print(id(z))
# z=a+b
# print(id(z))
# 以上操作是不合理的，因为你会发现他们的地址值变了。正确的写法应该是下面这样：
# z=a+b
# print(id(z))
# # 直接通过z[:]=的操作来对里面的每个元素做操作，不会改变原来的地址值
# z[:]=a*b
# print(id(z))


